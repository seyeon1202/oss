{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEZdi6lVxlITKOQxuHYVxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyeon1202/oss/blob/main/oss_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRHepq0hw07J",
        "outputId": "5e57aac4-43ae-40c9-ff50-addb10a99a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv_rswZ1w4fh",
        "outputId": "47ba00c9-efbd-43a3-a7f6-e9511a2050d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "\n",
            "positional arguments:\n",
            "  file_or_dir\n",
            "\n",
            "general:\n",
            "  -k EXPRESSION         Only run tests which match the given substring expression. An expression is\n",
            "                        a Python evaluatable expression where all names are substring-matched\n",
            "                        against test names and their parent classes. Example: -k 'test_method or\n",
            "                        test_other' matches all test functions and classes whose name contains\n",
            "                        'test_method' or 'test_other', while -k 'not test_method' matches those that\n",
            "                        don't contain 'test_method' in their names. -k 'not test_method and not\n",
            "                        test_other' will eliminate the matches. Additionally keywords are matched to\n",
            "                        classes and functions containing extra names in their\n",
            "                        'extra_keyword_matches' set, as well as functions which have names assigned\n",
            "                        directly to them. The matching is case-insensitive.\n",
            "  -m MARKEXPR           Only run tests matching given mark expression. For example: -m 'mark1 and\n",
            "                        not mark2'.\n",
            "  --markers             show markers (builtin, plugin and per-project ones).\n",
            "  -x, --exitfirst       Exit instantly on first error or failed test\n",
            "  --fixtures, --funcargs\n",
            "                        Show available fixtures, sorted by plugin appearance (fixtures with leading\n",
            "                        '_' are only shown with '-v')\n",
            "  --fixtures-per-test   Show fixtures per test\n",
            "  --pdb                 Start the interactive Python debugger on errors or KeyboardInterrupt\n",
            "  --pdbcls=modulename:classname\n",
            "                        Specify a custom interactive Python debugger for use with --pdb.For example:\n",
            "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
            "  --trace               Immediately break when running each test\n",
            "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
            "  -s                    Shortcut for --capture=no\n",
            "  --runxfail            Report the results of xfail tests as if they were not marked\n",
            "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all if none failed)\n",
            "  --ff, --failed-first  Run all tests, but run the last failures first. This may re-order tests and\n",
            "                        thus lead to repeated fixture setup/teardown.\n",
            "  --nf, --new-first     Run tests from new files first, then the rest of the tests sorted by file\n",
            "                        mtime\n",
            "  --cache-show=[CACHESHOW]\n",
            "                        Show cache contents, don't perform collection or tests. Optional argument:\n",
            "                        glob (default: '*').\n",
            "  --cache-clear         Remove all cache contents at start of test run\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        With ``--lf``, determines whether to execute tests when there are no\n",
            "                        previously (known) failures or when no cached ``lastfailed`` data was found.\n",
            "                        ``all`` (the default) runs the full test suite again. ``none`` just emits a\n",
            "                        message about no known failures and exits successfully.\n",
            "  --sw, --stepwise      Exit on test failure and continue from last failing test next time\n",
            "  --sw-skip, --stepwise-skip\n",
            "                        Ignore the first failing test but stop on the next failing test. Implicitly\n",
            "                        enables --stepwise.\n",
            "\n",
            "Reporting:\n",
            "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
            "  --durations-min=N     Minimal duration in seconds for inclusion in slowest list. Default: 0.005.\n",
            "  -v, --verbose         Increase verbosity\n",
            "  --no-header           Disable header\n",
            "  --no-summary          Disable summary\n",
            "  -q, --quiet           Decrease verbosity\n",
            "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
            "  -r chars              Show extra test summary info as specified by chars: (f)ailed, (E)rror,\n",
            "                        (s)kipped, (x)failed, (X)passed, (p)assed, (P)assed with output, (a)ll\n",
            "                        except passed (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
            "                        --disable-warnings), 'N' can be used to reset the list. (default: 'fE').\n",
            "  --disable-warnings, --disable-pytest-warnings\n",
            "                        Disable warnings summary\n",
            "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
            "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed through addopts)\n",
            "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
            "  --show-capture={no,stdout,stderr,log,all}\n",
            "                        Controls how captured stdout/stderr/log is shown on failed tests. Default:\n",
            "                        all.\n",
            "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
            "  --color=color         Color terminal output (yes/no/auto)\n",
            "  --code-highlight={yes,no}\n",
            "                        Whether code should be highlighted (only if --color is also enabled).\n",
            "                        Default: yes.\n",
            "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
            "  --junit-xml=path      Create junit-xml style report file at given path\n",
            "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
            "\n",
            "pytest-warnings:\n",
            "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
            "                        Set which warnings to report, see -W option of Python itself\n",
            "  --maxfail=num         Exit after first num failures or errors\n",
            "  --strict-config       Any warnings encountered while parsing the `pytest` section of the\n",
            "                        configuration file raise errors\n",
            "  --strict-markers      Markers not registered in the `markers` section of the configuration file\n",
            "                        raise errors\n",
            "  --strict              (Deprecated) alias to --strict-markers\n",
            "  -c FILE, --config-file=FILE\n",
            "                        Load configuration from `FILE` instead of trying to locate one of the\n",
            "                        implicit configuration files.\n",
            "  --continue-on-collection-errors\n",
            "                        Force test execution even if collection errors occur\n",
            "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path: 'root_dir',\n",
            "                        './root_dir', 'root_dir/another_dir/'; absolute path: '/home/user/root_dir';\n",
            "                        path with variables: '$HOME/root_dir'.\n",
            "\n",
            "collection:\n",
            "  --collect-only, --co  Only collect tests, don't execute them\n",
            "  --pyargs              Try to interpret all arguments as Python packages\n",
            "  --ignore=path         Ignore path during collection (multi-allowed)\n",
            "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
            "  --deselect=nodeid_prefix\n",
            "                        Deselect item (via node id prefix) during collection (multi-allowed)\n",
            "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
            "  --noconftest          Don't load any conftest.py files\n",
            "  --keep-duplicates     Keep duplicate tests\n",
            "  --collect-in-virtualenv\n",
            "                        Don't ignore tests in a local virtualenv directory\n",
            "  --import-mode={prepend,append,importlib}\n",
            "                        Prepend/append to sys.path when importing test modules and conftest files.\n",
            "                        Default: prepend.\n",
            "  --doctest-modules     Run doctests in all .py modules\n",
            "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
            "                        Choose another output format for diffs on doctest failure\n",
            "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
            "  --doctest-ignore-import-errors\n",
            "                        Ignore doctest ImportErrors\n",
            "  --doctest-continue-on-failure\n",
            "                        For a given doctest, continue to run after the first failure\n",
            "\n",
            "test session debugging and configuration:\n",
            "  --basetemp=dir        Base temporary directory for this test run. (Warning: this directory is\n",
            "                        removed if it exists.)\n",
            "  -V, --version         Display pytest version and information about plugins. When given twice, also\n",
            "                        display information about plugins.\n",
            "  -h, --help            Show help message and configuration info\n",
            "  -p name               Early-load given plugin module name or entry point (multi-allowed). To avoid\n",
            "                        loading of plugins, use the `no:` prefix, e.g. `no:doctest`.\n",
            "  --trace-config        Trace considerations of conftest.py files\n",
            "  --debug=[DEBUG_FILE_NAME]\n",
            "                        Store internal tracing debug information in this log file. This file is\n",
            "                        opened with 'w' and truncated as a result, care advised. Default:\n",
            "                        pytestdebug.log.\n",
            "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
            "                        Override ini option with \"option=value\" style, e.g. `-o xfail_strict=True -o\n",
            "                        cache_dir=cache`.\n",
            "  --assert=MODE         Control assertion debugging tools.\n",
            "                        'plain' performs no assertion debugging.\n",
            "                        'rewrite' (the default) rewrites assert statements in test modules on import\n",
            "                        to provide assert expression information.\n",
            "  --setup-only          Only setup fixtures, do not execute tests\n",
            "  --setup-show          Show setup of fixtures while executing tests\n",
            "  --setup-plan          Show what fixtures and tests would be executed but don't execute anything\n",
            "\n",
            "logging:\n",
            "  --log-level=LEVEL     Level of messages to catch/display. Not set by default, so it depends on the\n",
            "                        root/parent log handler's effective level, where it is \"WARNING\" by default.\n",
            "  --log-format=LOG_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-date-format=LOG_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-cli-level=LOG_CLI_LEVEL\n",
            "                        CLI logging level\n",
            "  --log-cli-format=LOG_CLI_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
            "  --log-file-level=LOG_FILE_LEVEL\n",
            "                        Log file logging level\n",
            "  --log-file-format=LOG_FILE_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-auto-indent=LOG_AUTO_INDENT\n",
            "                        Auto-indent multiline messages passed to the logging module. Accepts\n",
            "                        true|on, false|off or an integer.\n",
            "  --log-disable=LOGGER_DISABLE\n",
            "                        Disable a logger by name. Can be passed multiple times.\n",
            "\n",
            "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
            "\n",
            "  markers (linelist):   Markers for test functions\n",
            "  empty_parameter_set_mark (string):\n",
            "                        Default marker for empty parametersets\n",
            "  norecursedirs (args): Directory patterns to avoid for recursion\n",
            "  testpaths (args):     Directories to search for tests when no files or directories are given on\n",
            "                        the command line\n",
            "  filterwarnings (linelist):\n",
            "                        Each line specifies a pattern for warnings.filterwarnings. Processed after\n",
            "                        -W/--pythonwarnings.\n",
            "  usefixtures (args):   List of default fixtures to be used with this project\n",
            "  python_files (args):  Glob-style file patterns for Python test module discovery\n",
            "  python_classes (args):\n",
            "                        Prefixes or glob names for Python test class discovery\n",
            "  python_functions (args):\n",
            "                        Prefixes or glob names for Python test function and method discovery\n",
            "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
            "                        Disable string escape non-ASCII characters, might cause unwanted side\n",
            "                        effects(use at your own risk)\n",
            "  console_output_style (string):\n",
            "                        Console output: \"classic\", or with additional progress information\n",
            "                        (\"progress\" (percentage) | \"count\" | \"progress-even-when-capture-no\" (forces\n",
            "                        progress even when capture=no)\n",
            "  xfail_strict (bool):  Default for the strict parameter of xfail markers when not given explicitly\n",
            "                        (default: False)\n",
            "  tmp_path_retention_count (string):\n",
            "                        How many sessions should we keep the `tmp_path` directories, according to\n",
            "                        `tmp_path_retention_policy`.\n",
            "  tmp_path_retention_policy (string):\n",
            "                        Controls which directories created by the `tmp_path` fixture are kept\n",
            "                        around, based on test outcome. (all/failed/none)\n",
            "  enable_assertion_pass_hook (bool):\n",
            "                        Enables the pytest_assertion_pass hook. Make sure to delete any previously\n",
            "                        generated pyc cache files.\n",
            "  junit_suite_name (string):\n",
            "                        Test suite name for JUnit report\n",
            "  junit_logging (string):\n",
            "                        Write captured log messages to JUnit report: one of\n",
            "                        no|log|system-out|system-err|out-err|all\n",
            "  junit_log_passing_tests (bool):\n",
            "                        Capture log information for passing tests to JUnit report:\n",
            "  junit_duration_report (string):\n",
            "                        Duration time to report: one of total|call\n",
            "  junit_family (string):\n",
            "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
            "  doctest_optionflags (args):\n",
            "                        Option flags for doctests\n",
            "  doctest_encoding (string):\n",
            "                        Encoding used for doctest files\n",
            "  cache_dir (string):   Cache directory path\n",
            "  log_level (string):   Default value for --log-level\n",
            "  log_format (string):  Default value for --log-format\n",
            "  log_date_format (string):\n",
            "                        Default value for --log-date-format\n",
            "  log_cli (bool):       Enable log display during test run (also known as \"live logging\")\n",
            "  log_cli_level (string):\n",
            "                        Default value for --log-cli-level\n",
            "  log_cli_format (string):\n",
            "                        Default value for --log-cli-format\n",
            "  log_cli_date_format (string):\n",
            "                        Default value for --log-cli-date-format\n",
            "  log_file (string):    Default value for --log-file\n",
            "  log_file_level (string):\n",
            "                        Default value for --log-file-level\n",
            "  log_file_format (string):\n",
            "                        Default value for --log-file-format\n",
            "  log_file_date_format (string):\n",
            "                        Default value for --log-file-date-format\n",
            "  log_auto_indent (string):\n",
            "                        Default value for --log-auto-indent\n",
            "  pythonpath (paths):   Add paths to sys.path\n",
            "  faulthandler_timeout (string):\n",
            "                        Dump the traceback of all threads if a test takes more than TIMEOUT seconds\n",
            "                        to finish\n",
            "  addopts (args):       Extra command line options\n",
            "  minversion (string):  Minimally required pytest version\n",
            "  required_plugins (args):\n",
            "                        Plugins that must be present for pytest to run\n",
            "\n",
            "Environment variables:\n",
            "  PYTEST_ADDOPTS           Extra command line options\n",
            "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
            "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
            "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
            "\n",
            "\n",
            "to see available markers type: pytest --markers\n",
            "to see available fixtures type: pytest --fixtures\n",
            "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "def test_file1_method1():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x+1 ==y,\"test failed\"\n",
        "  assert x==y, \"test failed\"\n",
        "def test_file1_method2():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x+1 ==y, \"test failed\""
      ],
      "metadata": {
        "id": "AING9mHTxHAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVY4b0Q_yB-a",
        "outputId": "72f07114-6578-4ec0-a716-f0056247c6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 0 items                                                                                  \u001b[0m\n",
            "\n",
            "\u001b[33m====================================== \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =======================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "def test_file1_method1():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x+1 ==y,\"test failed\"\n",
        "  assert x==y, \"test failed\"\n",
        "def test_file1_method2():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x+1 ==y, \"test failed\""
      ],
      "metadata": {
        "id": "vc8xtYJYyPq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "@pytest.mark.parametrize(\"input1,input2,output\",[(5,5,10),(3,5,12)])\n",
        "def test_add(input1,input2,output):\n",
        "  assert input1+input2 == output,\"failed\""
      ],
      "metadata": {
        "id": "3EP0RPjt3cPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}